---
layout: post
title:  "Encoding"
date:   2018-04-27
categories:
---
This post is about code and coding, but not in the sense of "morse code" not "source code". This may sound like a fairly niche topic, only relevant to those interested in cryptography, compression, or networking, but instead I think it is a very general topic and one which is important in all areas of software development. A code is a mapping between information and a concrete representation of that information. This definition encompasses other terms such as encoding, parsing, serialization, and data "formats". Since computers only operate on bits, all information must be encoded into a representation as bits for it to be processed by a computer — computers don't operate on abstractions. This means that as a programmer you are constantly working with codes, whether or not you are always aware of it.

This has been pretty abstract so far so let's shift to an example: writing a function to decode/deserialize data representing a user into a python object. Here's the skeleton:
```py
import json

class User:
  def __init__(self, name, signed_up_at):
    self.name = name
    self.signed_up_at = signed_up_at

  @classmethod
  def decode_from_bytes(cls, encoded_bytes):
    # TODO: Decode name and signed_up_at from the bytes.
    return cls(name, signed_up_at) # Return an instance of the class.
```
A `User` instance has two attributes: `name`, and `signed_up_at`. The `decode_from_bytes` method needs to decode these from the bytes passed to it and create an instance with the attributes. Can we fill in the implementation now? Of course without knowing anything more we can only guess at how to decode the information. What we need to know is the code.

If we inspect an example of `encoded_bytes` we can probably make some guesses about the code:
```
>>> encoded_bytes
b'{"first_name": "Chris", "signed_up_at": 1503892800.0, "last_name": "Stadler"}'
```
It looks like the data is encoded as json. Let's try to use python's `json` library to decode the data into a dictionary:
```
>>> json.loads(encoded_bytes)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/usr/lib/python3.5/json/__init__.py", line 312, in loads
    s.__class__.__name__))
TypeError: the JSON object must be str, not 'bytes'
```
We've skipped a step: decoding the bytes into text. When we printed the `encoded_bytes` object above python decoded it as ASCII to provide a more human readable representation than a list of integers or a string of 1s and 0s. We need to specify the encoding explicitly though to get a `str` object:
```
>>> encoded_bytes.decode('ascii')
'{"first_name": "Chris", "signed_up_at": 1503892800.0, "last_name": "Stadler"}'
```
Awesome, now we have a string that we can pass to `json.loads()`. Let's try the same thing with data for another user:
```
>>> encoded_bytes2.decode('ascii')
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
UnicodeDecodeError: 'ascii' codec can't decode byte 0xc3 in position 50: ordinal not in range(128)
```
Uh oh, it looks like `encoded_bytes2` isn't valid ASCII. How do we know what character encoding was used to encode it? We could try decoding it using some other encodings (trial and error), if we know about how some character encodings work we could look at the actual bytes and make an educated guess, or we could use a program to automate this guessing. With any of these methods though how do you know if you've found the correct encoding?

When we were just looking at the first user's bytes it seemed like ASCII worked, but this was just because the true encoding uses the same codes as ASCII for some characters. We can test more instances of encoded user data, but how can we ever be confident that the encoding will work for all possible users? Furthermore, even if we can decode a user's data using a given encoding without an error being raised, how do we know that the resulting information is the same as the information which was decoded? We run into this if we try to decode our second user using the Latin-1 character encoding:
```
>>> encoded_bytes2.decode('latin-1')
'{"signed_up_at": 1503892800.0, "first_name": "FranÃ§ois", "last_name": "NegrÃ©"}'
```
We don't get an error like we did for ASCII, but the output is clearly wrong — we have not successfully recovered the encoded information. In this case this is fairly obvious because we know what characters tend to occur in names, but this may not always be the case. In general, given only instances of encoded data, there is no way to verify that we are using the correct code — the code must be known in advance.

In this case the correct character encoding is UTF-8:
```
>>> json_string = encoded_bytes2.decode('utf-8')
>>> json_string
'{"signed_up_at": 1503892800.0, "first_name": "François", "last_name": "Negré"}'
```


Huffman codes for compression.
Hamming codes for error-correction.

What is a file extension? Everyone who uses computers is familiar with them (although this is maybe less true now that many people's primary computing environment is a smart phone), but I think there is a widespread misconception about what they are. I used to think that the extension was somehow intrinsic to the file, and determined what program could open it. I assumed that if you changed the extension the computer would probably explode, or something like that. I've seen this belief in others as well, often when they are having trouble opening a file. For example, maybe they have a `.csv` file from work and they just need to edit one thing, but they don't have Excel on their personal computer. I suggest that they open it in a text editor. This is practically nonsensical to them because "it's not a text file", but when they go ahead and try (maybe needing changing the extension to `.txt` first) it works! Sort of... of course it doesn't look like it does in Excel, but that it can be opened at all is surprising to them (this makes for a great party trick).

This trick illustrates two concepts: file extensions are metadata about the encoding of the file, and encodings are often layered. If we tell a text editor that the file is encoded as text (either by changing the extension, or telling it to open the file regardless of extension) then it will attempt to open it. Of course you can't do this for any kind of file and get a useful result; it works for CSVs (and many other kinds of files) because they are encoded into text. Even though the text editor may not know anything about how CSVs are structured (and so can't render the data as a spreadsheet) it can still decode the file into its "raw" text form.

"Encoding" is a generally applicable concept which I think is under appreciated. Even for me when I think of the word "encoding" the first things that come to mind are character encodings like "utf-8" and "Windows-1252", and the memory of trying to clean data which had been corrupted by the use of conflicting encodings over many years. But the way that I want to use "encoding" here is not restricted to character encodings. Instead, whenever any kind of information needs to be stored or transmitted some kind of encoding is required.

A code is a mapping between information a concrete representation. The code must be know for the encoded information to be recovered.

If you are building a representation of information without thinking about the encoding (whether or not you call it that) — without recognizing that it is distinct from the information itself — then the encoding you come up with is unlikely to be ideal for the task.

What makes a good encoding?

It seems strange strange that there is no standard way to indicate the character encoding of a text file via metadata — why don't we have `.utf-8` files instead of just generic `.txt`? In the absence of such metadata programs often attempt to guess the encoding based on the content, which is often error prone.

utf-8: bytes -> text
json: text -> generic hash/dict
schema: hash/dict -> application object

There are even lower levels on the encoding stack, like how bits are physically encoded.
